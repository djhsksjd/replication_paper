
# 线性回归算法实现

本文件夹包含线性回归算法的Python和PyTorch实现，以及数据生成器。

## 算法简介
线性回归是一种基本的监督学习算法，用于建模自变量（特征）和因变量（目标值）之间的线性关系。它假设输入特征和输出之间存在线性关系，可以表示为：


$y = w_1*x_1 + w_2*x_2 + ... + w_n*x_n + b$


其中，w是权重，b是偏置。

## 文件夹结构
- `data_generator.py`: 数据生成器，用于生成线性回归的训练和测试数据
- `linear_regression_python.py`: 纯Python实现的线性回归算法（使用梯度下降）
- `linear_regression_pytorch.py`: PyTorch实现的线性回归算法
- `data/`: 存放生成的数据文件
- `readme.md`: 算法说明文档

## 环境依赖
- Python 3.6+
- numpy
- scikit-learn (用于评估模型性能)
- matplotlib (用于可视化)
- PyTorch (仅用于PyTorch实现)

## 使用步骤

### 1. 生成数据
首先运行数据生成器来创建训练和测试数据：

```bash
python data_generator.py
```

这将在`data`文件夹中生成`X.npy`和`y.npy`文件，分别包含特征数据和目标值。

### 2. 运行Python实现的线性回归

```bash
python linear_regression_python.py
```

### 3. 运行PyTorch实现的线性回归

```bash
python linear_regression_pytorch.py
```

## 参数说明

### 数据生成器参数
- `n_samples`: 样本数量，默认为1000
- `n_features`: 特征数量，默认为1
- `noise_level`: 噪声水平，默认为0.1
- `random_state`: 随机种子，默认为42

### 模型参数

#### Python实现
- `learning_rate`: 学习率，默认为0.01
- `n_iters`: 迭代次数，默认为1000

#### PyTorch实现
- `input_dim`: 输入特征维度，需要根据数据设置
- `learning_rate`: 学习率，默认为0.01
- `n_iters`: 迭代次数，默认为1000
- `batch_size`: 批次大小，默认为32

## 模型评估指标
- `MSE`: 均方误差，衡量预测值与真实值之间的平均平方差
- `R²`: 决定系数，衡量模型对数据变异的解释程度

## 可视化输出
- `loss_plot.png`/`loss_plot_pytorch.png`: 损失函数随迭代次数的变化曲线
- `predictions_plot.png`/`predictions_plot_pytorch.png`: 预测值与真实值的对比图（仅当特征维度为1时）


要理解线性回归，需从**核心定义→数学模型→损失函数→优化方法→正则化（防过拟合）** 层层递进，以下结合数学原理和实际意义详细拆解：


## 一、线性回归的核心：建模“线性关系”
线性回归是一种**监督学习算法**，核心目标是：找到自变量（特征）与因变量（标签）之间的**线性映射关系**，用于预测连续型输出（如房价、销售额、温度）。


### 1. 两种基本形式
根据自变量的数量，线性回归分为“简单”和“多元”，本质是“特征维度”的差异。

#### （1）简单线性回归（1个特征）
当仅用1个特征（如“房屋面积”）预测标签（如“房价”）时，模型假设两者满足**一元一次函数关系**：  
数学公式：  
$$ y = wx + b $$  
- $y$：因变量（预测值，如房价）；  
- $x$：自变量（特征，如房屋面积）；  
- $w$：权重（斜率，代表“x每增加1单位，y平均增加w单位”，如面积每多10㎡，房价多50万，则$w=5$）；  
- $b$：偏置（截距，代表“x=0时y的基准值”，如面积为0时的基础成本，实际中需结合业务理解）。

**示例**：用“面积（x）”预测“房价（y）”，模型最终可能学到$y=5x + 10$（面积每多10㎡，房价多50万，基础成本10万）。


#### （2）多元线性回归（多个特征）
当用多个特征（如“面积、房间数、地段评分”）预测标签时，模型扩展为**多元一次函数**，核心是“特征的线性组合”：  
数学公式（向量形式，更简洁）：  
$$ y = \mathbf{w}^T \mathbf{x} + b $$  
- $\mathbf{x} = [x_1, x_2, ..., x_p]^T$：$p$维特征向量（如$x_1$=面积，$x_2$=房间数，$x_3$=地段评分）；  
- $\mathbf{w} = [w_1, w_2, ..., w_p]^T$：$p$维权重向量（每个$w_i$代表“第i个特征对y的影响程度”，如$w_1=5$表示面积的影响，$w_3=20$表示地段的影响更大）；  
- $\mathbf{w}^T \mathbf{x}$：向量内积（即$w_1x_1 + w_2x_2 + ... + w_px_p$，特征与权重的线性组合）；  
- $b$：偏置（所有特征为0时，y的基准值）。

**矩阵形式（批量数据）**：若有$n$个样本，可写成：  
$$ \mathbf{Y} = \mathbf{X}\mathbf{W} $$  
- $\mathbf{Y} = [y_1, y_2, ..., y_n]^T$：$n$维标签向量；  
- $\mathbf{X} = \begin{bmatrix} 1 & x_{11} & x_{12} & ... & x_{1p} \\ 1 & x_{21} & x_{22} & ... & x_{2p} \\ ... & ... & ... & ... & ... \\ 1 & x_{n1} & x_{n2} & ... & x_{np} \end{bmatrix}$：$n×(p+1)$维设计矩阵（首列全为1，对应偏置$b$，可将$b$纳入$\mathbf{W}$成为$p+1$维权重向量）；  
- $\mathbf{W} = [b, w_1, w_2, ..., w_p]^T$：$p+1$维权重向量（含偏置）。


### 2. 线性回归的核心假设
模型有效成立需满足4个关键假设（若假设不满足，需调整模型或数据）：  
1. **线性性**：自变量与因变量的关系是“线性的”（若为非线性，需用多项式回归、非线性模型）；  
2. **误差独立性**：样本间的误差（实际值与预测值的差）相互独立（如第1个房子的误差不影响第2个）；  
3. **误差同方差性**：误差的方差恒定（不随自变量变化，若方差随x增大而增大，需用加权最小二乘）；  
4. **误差正态分布**：误差服从$N(0, \sigma^2)$（保证普通最小二乘（OLS）是“最优无偏估计”，即Gauss-Markov定理）。


### 3. 损失函数：衡量“预测误差”
线性回归的目标是找到最优$\mathbf{w}$和$b$，使“预测值与实际值的误差最小”。常用**均方误差（MSE）** 作为损失函数：  

#### （1）MSE的定义
对$n$个样本，MSE是“所有样本误差平方的平均值”：  
$$ \mathcal{L}(\mathbf{w}, b) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 $$  
- $y_i$：第$i$个样本的实际标签；  
- $\hat{y}_i = \mathbf{w}^T \mathbf{x}_i + b$：第$i$个样本的预测标签；  
- 平方项：① 放大较大误差（惩罚极端错误）；② 使损失函数可微（便于后续优化）。

#### （2）矩阵形式的MSE
结合批量数据的矩阵表示，MSE可简化为：  
$$ \mathcal{L}(\mathbf{W}) = \frac{1}{n} \|\mathbf{Y} - \mathbf{X}\mathbf{W}\|_2^2 $$  
- $\|\cdot\|_2$：L2范数（向量元素平方和的平方根），$\|\mathbf{Y} - \mathbf{X}\mathbf{W}\|_2^2$即所有误差的平方和。


## 二、线性回归的优化方法：求“最小损失”的参数
优化的核心是：在损失函数$\mathcal{L}$的“函数空间”中，找到使$\mathcal{L}$最小的$\mathbf{W}$。分为**闭式解（直接计算）** 和**迭代优化（逐步逼近）** 两类。


### 1. 闭式解：普通最小二乘（OLS）
当特征数$p$较小时（如$p<1000$），可直接通过数学推导得到最优$\mathbf{W}$，无需迭代。

#### （1）OLS的推导（矩阵形式）
对MSE损失函数$\mathcal{L}(\mathbf{W}) = \frac{1}{n} \|\mathbf{Y} - \mathbf{X}\mathbf{W}\|_2^2$求导，令导数为0（极值条件）：  
1. 展开损失函数：$\mathcal{L} = \frac{1}{n} (\mathbf{Y} - \mathbf{X}\mathbf{W})^T (\mathbf{Y} - \mathbf{X}\mathbf{W})$；  
2. 对$\mathbf{W}$求偏导：$\frac{\partial \mathcal{L}}{\partial \mathbf{W}} = \frac{2}{n} \mathbf{X}^T (\mathbf{X}\mathbf{W} - \mathbf{Y})$；  
3. 令导数为0，解出$\mathbf{W}$：  
$$ \hat{\mathbf{W}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y} $$  

这就是OLS的**闭式解**，$\hat{\mathbf{W}}$是最优权重向量（含偏置）。

#### （2）OLS的适用场景与局限性
- **适用场景**：特征维度$p$小（如$p<10^4$），样本数$n$适中（如$n>p$），此时$\mathbf{X}^T \mathbf{X}$可逆（满秩）。  
- **局限性**：  
  1. 当$p>n$（特征数多于样本数）时，$\mathbf{X}^T \mathbf{X}$是奇异矩阵（不可逆），无闭式解；  
  2. 当$p$很大（如$p=10^5$）时，$\mathbf{X}^T \mathbf{X}$的逆计算复杂度为$O(p^3)$，计算量极大（如$p=10^5$时，$p^3=10^{15}$，无法实现）；  
  3. 若特征间存在多重共线性（如“面积”和“建筑面积”高度相关），$\mathbf{X}^T \mathbf{X}$的逆不稳定（权重波动大）。


### 2. 迭代优化方法：逐步逼近最优解
当OLS不适用时（如高维数据），需用迭代法：从初始$\mathbf{W}_0$出发，每次沿“损失函数下降最快的方向”（梯度反方向）更新$\mathbf{W}$，直到收敛。

#### （1）梯度下降（Gradient Descent, GD）：全样本更新
- **原理**：每次迭代用**所有样本**计算梯度，沿梯度反方向更新$\mathbf{W}$。  
- **更新公式**：  
  $$ \mathbf{W}_{t+1} = \mathbf{W}_t - \eta \cdot \frac{\partial \mathcal{L}}{\partial \mathbf{W}_t} $$  
  - $t$：迭代次数；  
  - $\eta$：学习率（步长，控制每次更新的幅度，需手动调参，过大震荡，过小收敛慢）；  
  - 梯度$\frac{\partial \mathcal{L}}{\partial \mathbf{W}_t} = \frac{2}{n} \mathbf{X}^T (\mathbf{X}\mathbf{W}_t - \mathbf{Y})$（与OLS推导的导数一致）。  
- **优缺点**：  
  - 优点：梯度稳定，收敛到全局最优（MSE是凸函数，无局部最优）；  
  - 缺点：每次需遍历所有样本，当$n$很大（如$n=10^6$）时，计算量极大，训练慢。


#### （2）随机梯度下降（Stochastic Gradient Descent, SGD）：单样本更新
- **原理**：每次迭代仅用**1个随机样本**计算梯度（用单个样本的误差近似全局误差），快速更新$\mathbf{W}$。  
- **更新公式**（对第$i$个随机样本）：  
  $$ \mathbf{W}_{t+1} = \mathbf{W}_t - \eta \cdot 2(x_i (x_i^T \mathbf{W}_t - y_i)) $$  
  - 用单个样本的梯度$\frac{\partial \mathcal{L}_i}{\partial \mathbf{W}} = 2x_i (x_i^T \mathbf{W} - y_i)$替代全样本梯度。  
- **优缺点**：  
  - 优点：训练速度快（每次仅1个样本），适合大数据集；  
  - 缺点：梯度波动大（单样本误差不稳定），损失函数震荡下降，可能在最优解附近徘徊，不收敛到精确值。


#### （3）小批量梯度下降（Mini-Batch Gradient Descent, MBGD）：折中方案
- **原理**：每次迭代用**一小批样本**（如32、64、128个）计算梯度，平衡“GD的稳定性”和“SGD的速度”。  
- **更新公式**（对第$b$批样本，共$m$个样本）：  
  $$ \mathbf{W}_{t+1} = \mathbf{W}_t - \eta \cdot \frac{2}{m} \mathbf{X}_b^T (\mathbf{X}_b \mathbf{W}_t - \mathbf{Y}_b) $$  
  - $\mathbf{X}_b$：第$b$批的设计矩阵（$m×(p+1)$）；$\mathbf{Y}_b$：第$b$批的标签向量。  
- **优缺点**：  
  - 优点：梯度较稳定（批量平滑噪声），训练速度快（批量并行计算），是工业界最常用的基础优化方法；  
  - 缺点：需调参“批量大小”（太小震荡，太大计算慢）。


#### （4）改进的梯度下降：加速收敛与稳定
为解决基础GD/SGD的“收敛慢”“震荡”问题，衍生出以下优化器：

| 优化器       | 核心改进                                                                 | 优点                                  | 适用场景                  |
|--------------|--------------------------------------------------------------------------|---------------------------------------|---------------------------|
| 动量法（Momentum） | 引入“动量项”（前几次梯度的加权平均），模拟物理惯性，减少震荡，加速收敛。 | 缓解SGD的震荡，加速高曲率区域收敛      | 损失函数表面崎岖的场景    |
| 自适应学习率（Adagrad） | 为每个权重分配“自适应学习率”（频繁更新的权重用小学习率，反之用大学习率）。 | 适合稀疏数据（如文本特征），无需手动调学习率 | 稀疏特征、参数更新频率差异大 |
| RMSprop      | 改进Adagrad（避免学习率随迭代衰减至0），用“指数移动平均”替代累积平方梯度。 | 解决Adagrad的学习率衰减问题，稳定收敛  | 大多数深度学习场景        |
| Adam         | 结合“动量法”（梯度方向）和“RMSprop”（自适应学习率），兼顾稳定与速度。     | 收敛快、稳定，鲁棒性强                | 几乎所有场景（工业界首选）|


## 三、正则化方法：防止线性回归的“过拟合”
### 1. 为什么需要正则化？
多元线性回归中，若特征数$p$过多（如$p=1000$，样本$n=100$），模型会“过度拟合训练数据的噪声”，表现为：  
- 训练集MSE很小（预测极准）；  
- 测试集MSE很大（泛化能力差）。  

正则化的核心是：在损失函数中加入“权重惩罚项”，**限制权重的绝对值大小**，迫使模型选择“简单的线性组合”（避免过度依赖某几个特征的噪声）。


### 2. 三种常用正则化方法
#### （1）L2正则化：Ridge回归（岭回归）
- **数学形式**：在MSE损失函数中加入“权重的L2范数平方”（即$\|\mathbf{w}\|_2^2 = w_1^2 + w_2^2 + ... + w_p^2$）：  
  $$ \mathcal{L}_{\text{Ridge}} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 + \lambda \cdot \|\mathbf{w}\|_2^2 $$  
  - $\lambda \geq 0$：正则化强度（$\lambda=0$时退化为普通线性回归；$\lambda$越大，惩罚越强，权重越接近0）；  
  - 注意：偏置$b$通常不惩罚（因为$b$是基准值，不直接影响特征的“相对重要性”）。

- **核心作用**：  
  1. **权重衰减**：迫使所有权重都趋近于0（但不会等于0），避免某几个权重过大（过度依赖某特征）；  
  2. **解决多重共线性**：当特征间高度相关时，OLS的权重会波动很大，Ridge通过惩罚权重，使权重更稳定；  
  3. **存在闭式解**：Ridge的最优权重为$\hat{\mathbf{W}} = (\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^T \mathbf{Y}$（$\mathbf{I}$是单位矩阵，保证$\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I}$可逆，即使$p>n$）。

- **几何意义**：在“权重空间”中，Ridge的优化是“MSE损失的等高线”与“L2范数的圆形约束域”的切点（圆形约束使权重分布更均匀）。


#### （2）L1正则化：Lasso回归（套索回归）
- **数学形式**：在MSE损失函数中加入“权重的L1范数”（即$\|\mathbf{w}\|_1 = |w_1| + |w_2| + ... + |w_p|$）：  
  $$ \mathcal{L}_{\text{Lasso}} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 + \lambda \cdot \|\mathbf{w}\|_1 $$  

- **核心作用**：  
  1. **特征选择**：L1正则化会使**部分权重等于0**（即该特征被模型“抛弃”），因为L1的约束域是菱形，与MSE等高线的切点更可能落在坐标轴上（某权重为0）；  
  2. **稀疏模型**：保留重要特征，剔除冗余/无关特征，使模型更易解释（如房价预测中，Lasso可能让“房间数”的权重非0，“窗户数量”的权重为0）；  
  3. **无闭式解**：L1范数在$w=0$处不可微（导数左负右正），需用**坐标下降（Coordinate Descent）** 或**近端梯度下降（Proximal Gradient Descent）** 求解。

- **几何意义**：Lasso的优化是“MSE等高线”与“L1范数的菱形约束域”的切点（菱形的顶点在坐标轴上，易使某权重为0）。


#### （3）弹性网络（Elastic Net）：结合L1与L2
- **数学形式**：同时加入L1和L2惩罚项，平衡两者的优势：  
  $$ \mathcal{L}_{\text{Elastic Net}} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 + \lambda_1 \cdot \|\mathbf{w}\|_1 + \lambda_2 \cdot \|\mathbf{w}\|_2^2 $$  
  或简化为（用$\alpha$控制L1/L2比例）：  
  $$ \mathcal{L} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 + \lambda \left( (1-\alpha) \cdot \frac{1}{2}\|\mathbf{w}\|_2^2 + \alpha \cdot \|\mathbf{w}\|_1 \right) $$  
  - $\alpha \in [0,1]$：$\alpha=1$时退化为Lasso，$\alpha=0$时退化为Ridge；  
  - $\lambda$：总正则化强度。

- **核心作用**：  
  - 解决Lasso的缺陷：当$p$远大于$n$或特征高度相关时，Lasso可能随机选择其中一个特征（不稳定），Elastic Net通过L2的权重衰减，使相关特征的权重更合理；  
  - 保留L1的特征选择能力：同时避免单特征过度影响，是高维、强相关数据的首选正则化方法。


### 3. 正则化强度$\lambda$的选择
$\lambda$是关键超参数，需通过**交叉验证（Cross-Validation）** 选择：  
1. 将训练集分为$k$份（如$k=5$，即5折交叉验证）；  
2. 对每个候选$\lambda$（如$\lambda \in [0.001, 0.01, 0.1, 1, 10]$）：  
   - 用$k-1$份数据训练模型；  
   - 用1份数据验证（计算验证集MSE）；  
3. 选择“平均验证MSE最小”的$\lambda$，用该$\lambda$训练全量训练集，最终评估测试集性能。


## 四、总结：线性回归的核心逻辑链
1. **建模目标**：用特征的线性组合拟合连续型标签；  
2. **损失函数**：MSE（衡量误差，可微且惩罚极端错误）；  
3. **优化方法**：  
   - 低维数据：OLS闭式解（直接计算，高效）；  
   - 高维/大数据：迭代法（MBGD、Adam等，平衡速度与稳定）；  
4. **正则化**：  
   - 防过拟合：通过惩罚权重限制模型复杂度；  
   - 选择依据：需特征选择用Lasso，需稳定权重用Ridge，高维强相关用Elastic Net；  
5. **超参选择**：用交叉验证确定$\lambda$，保证模型泛化能力。

线性回归是“最简单的回归模型”，但也是深度学习（如线性层）、梯度提升树（如XGBoost的线性基学习器）的基础，理解其原理对后续复杂模型的学习至关重要。